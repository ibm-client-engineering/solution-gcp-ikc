[
  {
    "objectID": "src/Deployment/2-CP4D.html",
    "href": "src/Deployment/2-CP4D.html",
    "title": "Cloud Pak for Data",
    "section": "",
    "text": "The CLI can be downloaded from the following site.\nhttps://github.com/IBM/cpd-cli/releases\n\n\n\nCreate a new file name cpd_vars.sh and add the following:\n#===============================================================================\n# Cloud Pak for Data installation variables\n#===============================================================================\n\n# ------------------------------------------------------------------------------\n# Client workstation \n# ------------------------------------------------------------------------------\n# Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export CPD_CLI_MANAGE_WORKSPACE=&lt;enter a fully qualified directory&gt;\n# export OLM_UTILS_LAUNCH_ARGS=&lt;enter launch arguments&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cluster\n# ------------------------------------------------------------------------------\n\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=&lt;enter your deployment type&gt;\nexport IMAGE_ARCH=&lt;enter your cluster architecture&gt;\n# export OCP_USERNAME=&lt;enter your username&gt;\n# export OCP_PASSWORD=&lt;enter your password&gt;\n# export OCP_TOKEN=&lt;enter your token&gt;\nexport SERVER_ARGUMENTS=\"--server=${OCP_URL}\"\n# export LOGIN_ARGUMENTS=\"--username=${OCP_USERNAME} --password=${OCP_PASSWORD}\"\n# export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\"\nexport CPDM_OC_LOGIN=\"cpd-cli manage login-to-ocp ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\nexport OC_LOGIN=\"oc login ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\n\n\n# ------------------------------------------------------------------------------\n# Proxy server\n# ------------------------------------------------------------------------------\n\n# export PROXY_HOST=&lt;enter your proxy server hostname&gt;\n# export PROXY_PORT=&lt;enter your proxy server port number&gt;\n# export PROXY_USER=&lt;enter your proxy server username&gt;\n# export PROXY_PASSWORD=&lt;enter your proxy server password&gt;\n\n\n# ------------------------------------------------------------------------------\n# Projects\n# ------------------------------------------------------------------------------\n\nexport PROJECT_CERT_MANAGER=&lt;enter your certificate manager project&gt;\nexport PROJECT_LICENSE_SERVICE=&lt;enter your License Service project&gt;\nexport PROJECT_SCHEDULING_SERVICE=&lt;enter your scheduling service project&gt;\n# export PROJECT_IBM_EVENTS=&lt;enter your IBM Events Operator project&gt;\n# export PROJECT_PRIVILEGED_MONITORING_SERVICE=&lt;enter your privileged monitoring service project&gt;\nexport PROJECT_CPD_INST_OPERATORS=&lt;enter your Cloud Pak for Data operator project&gt;\nexport PROJECT_CPD_INST_OPERANDS=&lt;enter your Cloud Pak for Data operand project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED=&lt;enter your tethered project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED_LIST=&lt;a comma-separated list of tethered projects&gt;\n\n\n\n# ------------------------------------------------------------------------------\n# Storage\n# ------------------------------------------------------------------------------\n\nexport STG_CLASS_BLOCK=&lt;RWO-storage-class-name&gt;\nexport STG_CLASS_FILE=&lt;RWX-storage-class-name&gt;\n\n# ------------------------------------------------------------------------------\n# IBM Entitled Registry\n# ------------------------------------------------------------------------------\n\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\n# ------------------------------------------------------------------------------\n# Private container registry\n# ------------------------------------------------------------------------------\n# Set the following variables if you mirror images to a private container registry.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cloud Pak for Data version\n# ------------------------------------------------------------------------------\n\nexport VERSION=5.1.3\n\n\n# ------------------------------------------------------------------------------\n# Components\n# ------------------------------------------------------------------------------\n\nexport COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_platform\n# export COMPONENTS_TO_SKIP=&lt;component-ID-1&gt;,&lt;component-ID-2&gt;\nThe following variables will need to be updated to include the OCP URL, OpenShift type (usually self-managed unless its on a specific cloud provider), and Image archetype such as “amd64”\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=self-managed\nexport IMAGE_ARCH=amd64\nSet the Project names for the different services, here are a few suggestions:\nexport PROJECT_CERT_MANAGER=ibm-cert-manager\nexport PROJECT_LICENSE_SERVICE=cpd-license-server\nexport PROJECT_SCHEDULING_SERVICE=cpd-scheduling\nexport PROJECT_CPD_INST_OPERATORS=cpd-operators\nexport PROJECT_CPD_INST_OPERANDS=cpd-instance\nSet your block storage class and file storage class, such as ocs-storagecluster-ceph-rbd and ocs-storagecluster-cephfs .\nexport STG_CLASS_BLOCK=ocs-storagecluster-ceph-rbd\nexport STG_CLASS_FILE=ocs-storagecluster-cephfs\nSet your private registry details:\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\nAdd your IBM Entitlement key to the following variable:\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\nRun the following command any time you update the environment variables file:\nsource cpd_vars.sh\n\n\n\n\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --password=${OCP_PASSWORD}\nor\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --token=${OCP_TOKEN}\n\n\n\ncpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key=${IBM_ENTITLEMENT_KEY}",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html#cloud-pak-for-data-install-preparation",
    "href": "src/Deployment/2-CP4D.html#cloud-pak-for-data-install-preparation",
    "title": "Cloud Pak for Data",
    "section": "",
    "text": "The CLI can be downloaded from the following site.\nhttps://github.com/IBM/cpd-cli/releases\n\n\n\nCreate a new file name cpd_vars.sh and add the following:\n#===============================================================================\n# Cloud Pak for Data installation variables\n#===============================================================================\n\n# ------------------------------------------------------------------------------\n# Client workstation \n# ------------------------------------------------------------------------------\n# Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export CPD_CLI_MANAGE_WORKSPACE=&lt;enter a fully qualified directory&gt;\n# export OLM_UTILS_LAUNCH_ARGS=&lt;enter launch arguments&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cluster\n# ------------------------------------------------------------------------------\n\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=&lt;enter your deployment type&gt;\nexport IMAGE_ARCH=&lt;enter your cluster architecture&gt;\n# export OCP_USERNAME=&lt;enter your username&gt;\n# export OCP_PASSWORD=&lt;enter your password&gt;\n# export OCP_TOKEN=&lt;enter your token&gt;\nexport SERVER_ARGUMENTS=\"--server=${OCP_URL}\"\n# export LOGIN_ARGUMENTS=\"--username=${OCP_USERNAME} --password=${OCP_PASSWORD}\"\n# export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\"\nexport CPDM_OC_LOGIN=\"cpd-cli manage login-to-ocp ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\nexport OC_LOGIN=\"oc login ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\n\n\n# ------------------------------------------------------------------------------\n# Proxy server\n# ------------------------------------------------------------------------------\n\n# export PROXY_HOST=&lt;enter your proxy server hostname&gt;\n# export PROXY_PORT=&lt;enter your proxy server port number&gt;\n# export PROXY_USER=&lt;enter your proxy server username&gt;\n# export PROXY_PASSWORD=&lt;enter your proxy server password&gt;\n\n\n# ------------------------------------------------------------------------------\n# Projects\n# ------------------------------------------------------------------------------\n\nexport PROJECT_CERT_MANAGER=&lt;enter your certificate manager project&gt;\nexport PROJECT_LICENSE_SERVICE=&lt;enter your License Service project&gt;\nexport PROJECT_SCHEDULING_SERVICE=&lt;enter your scheduling service project&gt;\n# export PROJECT_IBM_EVENTS=&lt;enter your IBM Events Operator project&gt;\n# export PROJECT_PRIVILEGED_MONITORING_SERVICE=&lt;enter your privileged monitoring service project&gt;\nexport PROJECT_CPD_INST_OPERATORS=&lt;enter your Cloud Pak for Data operator project&gt;\nexport PROJECT_CPD_INST_OPERANDS=&lt;enter your Cloud Pak for Data operand project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED=&lt;enter your tethered project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED_LIST=&lt;a comma-separated list of tethered projects&gt;\n\n\n\n# ------------------------------------------------------------------------------\n# Storage\n# ------------------------------------------------------------------------------\n\nexport STG_CLASS_BLOCK=&lt;RWO-storage-class-name&gt;\nexport STG_CLASS_FILE=&lt;RWX-storage-class-name&gt;\n\n# ------------------------------------------------------------------------------\n# IBM Entitled Registry\n# ------------------------------------------------------------------------------\n\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\n# ------------------------------------------------------------------------------\n# Private container registry\n# ------------------------------------------------------------------------------\n# Set the following variables if you mirror images to a private container registry.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cloud Pak for Data version\n# ------------------------------------------------------------------------------\n\nexport VERSION=5.1.3\n\n\n# ------------------------------------------------------------------------------\n# Components\n# ------------------------------------------------------------------------------\n\nexport COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_platform\n# export COMPONENTS_TO_SKIP=&lt;component-ID-1&gt;,&lt;component-ID-2&gt;\nThe following variables will need to be updated to include the OCP URL, OpenShift type (usually self-managed unless its on a specific cloud provider), and Image archetype such as “amd64”\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=self-managed\nexport IMAGE_ARCH=amd64\nSet the Project names for the different services, here are a few suggestions:\nexport PROJECT_CERT_MANAGER=ibm-cert-manager\nexport PROJECT_LICENSE_SERVICE=cpd-license-server\nexport PROJECT_SCHEDULING_SERVICE=cpd-scheduling\nexport PROJECT_CPD_INST_OPERATORS=cpd-operators\nexport PROJECT_CPD_INST_OPERANDS=cpd-instance\nSet your block storage class and file storage class, such as ocs-storagecluster-ceph-rbd and ocs-storagecluster-cephfs .\nexport STG_CLASS_BLOCK=ocs-storagecluster-ceph-rbd\nexport STG_CLASS_FILE=ocs-storagecluster-cephfs\nSet your private registry details:\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\nAdd your IBM Entitlement key to the following variable:\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\nRun the following command any time you update the environment variables file:\nsource cpd_vars.sh\n\n\n\n\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --password=${OCP_PASSWORD}\nor\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --token=${OCP_TOKEN}\n\n\n\ncpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key=${IBM_ENTITLEMENT_KEY}",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html#install-cp4d-pre-requisites",
    "href": "src/Deployment/2-CP4D.html#install-cp4d-pre-requisites",
    "title": "Cloud Pak for Data",
    "section": "Install CP4D Pre-Requisites",
    "text": "Install CP4D Pre-Requisites\n\nCertificate Manager & License Server\nThe following command will install the certificate manager and the License Server:\ncpd-cli manage apply-cluster-components \\\n--release=${VERSION} \\\n--license_acceptance=true \\\n--licensing_ns=${PROJECT_LICENSE_SERVICE}\n\n\nScheduling Service\nThe following command will install the Scheduling service:\ncpd-cli manage apply-scheduler \\\n--release=${VERSION} \\\n--license_acceptance=true \\\n--scheduler_ns=${PROJECT_SCHEDULING_SERVICE}\n\n\nCreate required namespaces\nThe following commands will create all the necessary namespaces:\noc new-project ${PROJECT_CPD_INST_OPERATORS}\noc new-project ${PROJECT_CPD_INST_OPERANDS}\n\n\nApply Permissions\nThe following commands will apply the required permissions for the CP4D Topology:\ncpd-cli manage authorize-instance-topology \\\n--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html#install-cloud-pak-for-data",
    "href": "src/Deployment/2-CP4D.html#install-cloud-pak-for-data",
    "title": "Cloud Pak for Data",
    "section": "Install Cloud Pak for Data",
    "text": "Install Cloud Pak for Data\n\nCloud Pak Foundation Services\nThe following command will install the Cloud Pak foundation services:\ncpd-cli manage setup-instance-topology \\\n--release=${VERSION} \\\n--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \\\n--license_acceptance=true \\\n--block_storage_class=${STG_CLASS_BLOCK}\n\n\nInstall operators\nThe following command will install the Cloud Pak for Data operators which are required to install services:\ncpd-cli manage apply-olm \\\n--release=${VERSION} \\\n--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \\\n--components=${COMPONENTS}\n\n\nInstall operands\nThe following command will install the Cloud Pak for Data base software.\ncpd-cli manage apply-cr \\\n--release=${VERSION} \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \\\n--components=cpd_platform \\\n--block_storage_class=${STG_CLASS_BLOCK} \\\n--file_storage_class=${STG_CLASS_FILE} \\\n--license_acceptance=true\n\n\nConfirm Operands are Completed:\ncpd-cli manage get-cr-status \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}\n\n\nRetrieve Login Details\nRun the following commands to get the Admin username and password for the Cloud Pak for Data install:\ncpd-cli manage get-cpd-instance-details \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \\\n--get_admin_initial_credentials=true",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html",
    "href": "src/solution_overview/troubleshooting.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Solution"
  },
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Best Practices\n(Capture the main takeaways and results of the project)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IBM Knowledge Catalog on GCP",
    "section": "",
    "text": "SL",
    "crumbs": [
      "IBM Knowledge Catalog on GCP"
    ]
  },
  {
    "objectID": "index.html#description-ibm-knowledge-catalog-on-gcp",
    "href": "index.html#description-ibm-knowledge-catalog-on-gcp",
    "title": "IBM Knowledge Catalog on GCP",
    "section": "Description: “IBM Knowledge Catalog on GCP”",
    "text": "Description: “IBM Knowledge Catalog on GCP”",
    "crumbs": [
      "IBM Knowledge Catalog on GCP"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "IBM Knowledge Catalog on GCP",
    "section": "The Why",
    "text": "The Why\n(High-level description of the business challenges and client pain points)\n\nProblem Details\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.\n\n\nAdditional Context\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.",
    "crumbs": [
      "IBM Knowledge Catalog on GCP"
    ]
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "src/solution_overview/prepare.html",
    "href": "src/solution_overview/prepare.html",
    "title": "Step One",
    "section": "",
    "text": "Step One Solution - The What\n(Summarize the overarching technical solution. Explain the reasoning and strategic considerations behind the solution)"
  },
  {
    "objectID": "src/solution_overview/01-Architecture.html",
    "href": "src/solution_overview/01-Architecture.html",
    "title": "Step Two",
    "section": "",
    "text": "Step Two Solution\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis.",
    "crumbs": [
      "Deployment",
      "Architecture"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html",
    "href": "src/Deployment/3-IKC.html",
    "title": "IBM Knowledge Catalog",
    "section": "",
    "text": "Go to OperatorHub\nSearch for “Node Feature Discovery”\nInstall\n\n\n\n\n\nGo to Installed Operators\nSelect “Node Feature Discovery”\nSelect the box “Provided APIs”\nSelect “Create Instance”\nReview the values\nSelect “Create”\n\n\n\n\n\nGo to OperatorHub\nSearch for “Nvidia GPU Operator”\nInstall\n\n\n\n\n\nGo to “Installed Operators”\nClick on “Nvidia GPU Operator”\nSelect “ClusterPolicy” tab\nClick “Create ClusterPolicy”\nClick “Create”\n\n\n\n\nCreate redhat-ods-operator project:\noc new-project redhat-ods-operator\nCreate the rhods-operator operator group in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nEOF\nCreate the rhods-operator operator subscription in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nspec:\n  name: rhods-operator\n  channel: stable-2.13\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n  config:\n     env:\n        - name: \"DISABLE_DSC_CONFIG\"\nEOF\nCreate a DSC Initialization (DSCInitialization) object named default-dsci in the redhat-ods-monitoring project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: dscinitialization.opendatahub.io/v1\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  applicationsNamespace: redhat-ods-applications\n  monitoring:\n    managementState: Managed\n    namespace: redhat-ods-monitoring\n  serviceMesh:\n    managementState: Removed\n  trustedCABundle:\n    managementState: Managed\n    customCABundle: \"\"\nEOF\nCheck the status of the rhods-operator-* pod in the redhat-ods-operator project:\noc get pods -n redhat-ods-operator\nConfirm that the pod is Running. The command returns a response with the following format:\nNAME                              READY   STATUS    RESTARTS   AGE\nrhods-operator-56c85d44c9-vtk74   1/1     Running   0          3h57m\nCheck the phase of the DSC Initialization (DSCInitialization) object:\noc get dscinitialization\nConfirm that the object is Ready. The command returns a response with the following format:\nNAME           AGE     PHASE\ndefault-dsci   4d18h   Ready\nCreate a Data Science Cluster (DataScienceCluster) object named default-dsc:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: datasciencecluster.opendatahub.io/v1\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    codeflare:\n      managementState: Removed\n    dashboard:\n      managementState: Removed\n    datasciencepipelines:\n      managementState: Removed\n    kserve:\n      managementState: Managed\n      defaultDeploymentMode: RawDeployment\n      serving:\n        managementState: Removed\n        name: knative-serving\n    kueue:\n      managementState: Removed\n    modelmeshserving:\n      managementState: Removed\n    ray:\n      managementState: Removed\n    trainingoperator:\n      managementState: Managed\n    trustyai:\n      managementState: Removed\n    workbenches:\n      managementState: Removed\nEOF\nWait for the Data Science Cluster object to be Ready. To check the status of the object, run:\noc get datasciencecluster default-dsc -o jsonpath='\"{.status.phase}\" {\"\\n\"}'\nConfirm that the status of the following pods in the redhat-ods-applications project are Running: kserve-controller-manager-* pod kubeflow-training-operator-* pod odh-model-controller-* pod\noc get pods -n redhat-ods-applications\nThe command returns a response with the following format:\nNAME                                         READY   STATUS      RESTARTS   AGE\nkserve-controller-manager-57796d5b44-sh9n5   1/1     Running     0          4m57s\nkubeflow-training-operator-7b99d5584c-rh5hb  1/1     Running     0          4m57s\nEdit the inferenceservice-config configuration map in the redhat-ods-applications project: Log in to the Red Hat OpenShift Container Platform web console as a cluster administrator. From the navigation menu, select Workloads &gt; Configmaps. From the Project list, select redhat-ods-applications. Click the inferenceservice-config resource. Then, open the YAML tab. In the metadata.annotations section of the file, add opendatahub.io/managed: ‘false’:\nmetadata:\n  annotations:\n    internal.config.kubernetes.io/previousKinds: ConfigMap\n    internal.config.kubernetes.io/previousNames: inferenceservice-config\n    internal.config.kubernetes.io/previousNamespaces: opendatahub\n    opendatahub.io/managed: 'false'\nFind the following entry in the file:\n\"domainTemplate\": \"{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}\",\nUpdate the value of the domainTemplate field to “example.com”:\n\"domainTemplate\": \"example.com\",\nclick save.",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html#pre-requirements",
    "href": "src/Deployment/3-IKC.html#pre-requirements",
    "title": "IBM Knowledge Catalog",
    "section": "",
    "text": "Go to OperatorHub\nSearch for “Node Feature Discovery”\nInstall\n\n\n\n\n\nGo to Installed Operators\nSelect “Node Feature Discovery”\nSelect the box “Provided APIs”\nSelect “Create Instance”\nReview the values\nSelect “Create”\n\n\n\n\n\nGo to OperatorHub\nSearch for “Nvidia GPU Operator”\nInstall\n\n\n\n\n\nGo to “Installed Operators”\nClick on “Nvidia GPU Operator”\nSelect “ClusterPolicy” tab\nClick “Create ClusterPolicy”\nClick “Create”\n\n\n\n\nCreate redhat-ods-operator project:\noc new-project redhat-ods-operator\nCreate the rhods-operator operator group in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nEOF\nCreate the rhods-operator operator subscription in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nspec:\n  name: rhods-operator\n  channel: stable-2.13\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n  config:\n     env:\n        - name: \"DISABLE_DSC_CONFIG\"\nEOF\nCreate a DSC Initialization (DSCInitialization) object named default-dsci in the redhat-ods-monitoring project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: dscinitialization.opendatahub.io/v1\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  applicationsNamespace: redhat-ods-applications\n  monitoring:\n    managementState: Managed\n    namespace: redhat-ods-monitoring\n  serviceMesh:\n    managementState: Removed\n  trustedCABundle:\n    managementState: Managed\n    customCABundle: \"\"\nEOF\nCheck the status of the rhods-operator-* pod in the redhat-ods-operator project:\noc get pods -n redhat-ods-operator\nConfirm that the pod is Running. The command returns a response with the following format:\nNAME                              READY   STATUS    RESTARTS   AGE\nrhods-operator-56c85d44c9-vtk74   1/1     Running   0          3h57m\nCheck the phase of the DSC Initialization (DSCInitialization) object:\noc get dscinitialization\nConfirm that the object is Ready. The command returns a response with the following format:\nNAME           AGE     PHASE\ndefault-dsci   4d18h   Ready\nCreate a Data Science Cluster (DataScienceCluster) object named default-dsc:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: datasciencecluster.opendatahub.io/v1\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    codeflare:\n      managementState: Removed\n    dashboard:\n      managementState: Removed\n    datasciencepipelines:\n      managementState: Removed\n    kserve:\n      managementState: Managed\n      defaultDeploymentMode: RawDeployment\n      serving:\n        managementState: Removed\n        name: knative-serving\n    kueue:\n      managementState: Removed\n    modelmeshserving:\n      managementState: Removed\n    ray:\n      managementState: Removed\n    trainingoperator:\n      managementState: Managed\n    trustyai:\n      managementState: Removed\n    workbenches:\n      managementState: Removed\nEOF\nWait for the Data Science Cluster object to be Ready. To check the status of the object, run:\noc get datasciencecluster default-dsc -o jsonpath='\"{.status.phase}\" {\"\\n\"}'\nConfirm that the status of the following pods in the redhat-ods-applications project are Running: kserve-controller-manager-* pod kubeflow-training-operator-* pod odh-model-controller-* pod\noc get pods -n redhat-ods-applications\nThe command returns a response with the following format:\nNAME                                         READY   STATUS      RESTARTS   AGE\nkserve-controller-manager-57796d5b44-sh9n5   1/1     Running     0          4m57s\nkubeflow-training-operator-7b99d5584c-rh5hb  1/1     Running     0          4m57s\nEdit the inferenceservice-config configuration map in the redhat-ods-applications project: Log in to the Red Hat OpenShift Container Platform web console as a cluster administrator. From the navigation menu, select Workloads &gt; Configmaps. From the Project list, select redhat-ods-applications. Click the inferenceservice-config resource. Then, open the YAML tab. In the metadata.annotations section of the file, add opendatahub.io/managed: ‘false’:\nmetadata:\n  annotations:\n    internal.config.kubernetes.io/previousKinds: ConfigMap\n    internal.config.kubernetes.io/previousNames: inferenceservice-config\n    internal.config.kubernetes.io/previousNamespaces: opendatahub\n    opendatahub.io/managed: 'false'\nFind the following entry in the file:\n\"domainTemplate\": \"{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}\",\nUpdate the value of the domainTemplate field to “example.com”:\n\"domainTemplate\": \"example.com\",\nclick save.",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html#installation",
    "href": "src/Deployment/3-IKC.html#installation",
    "title": "IBM Knowledge Catalog",
    "section": "Installation",
    "text": "Installation",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html#configuration",
    "href": "src/Deployment/3-IKC.html#configuration",
    "title": "IBM Knowledge Catalog",
    "section": "Configuration",
    "text": "Configuration",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  }
]