[
  {
    "objectID": "src/Deployment/1-OpenShift.html",
    "href": "src/Deployment/1-OpenShift.html",
    "title": "Step One",
    "section": "",
    "text": "Internet access for OpenShift Container Platform\nIn OpenShift Container Platform 4.14, you require access to the Internet to install your cluster.\nYou must have Internet access to:\n\nAccess the Red Hat OpenShift Cluster Manager page to download the installation program and perform subscription management. If the cluster has Internet access and you do not disable Telemetry, that service automatically entitles your cluster.\nAccess Quay.io to obtain the packages that are required to install your cluster.\nObtain the packages that are required to perform cluster updates.\nURLs listed below should be accessible from all the cluster nodes at port 443 and 80.\n\n\n\n\n\n\n\n\nURL\nFunction\n\n\n\n\nregistry.redhat.io\nProvides core container images\n\n\nquay.io\nProvides core container images\n\n\n*.quay.io\nProvides core container images\n\n\nsso.redhat.com\nThe https://console.redhat.com/openshift site uses authentication from sso.redhat.com\n\n\nopenshift.org\nProvides Red Hat Enterprise Linux CoreOS (RHCOS) images\n\n\nmirror.openshift.com\nRequired to access mirrored installation content and images. This site is also a source of release image signatures, although the Cluster Version Operator needs only a single functioning source.\n\n\nstorage.googleapis.com/openshift-release\nA source of release image signatures, although the Cluster Version Operator needs only a single functioning source.\n\n\napi.openshift.com\nRequired both for your cluster token and to check if updates are available for the cluster.\n\n\ncp.icr.io\nIBM Cloud Paks content\n\n\nicr.io\nIBM Cloud Paks content\n\n\ndocker.io\nIBM Cloud Paks content\n\n\ndd0.icr.io\nIBM Cloud Paks content\n\n\ndd2.icr.io\nIBM Cloud Paks content\n\n\ndd4.icr.io\nIBM Cloud Paks content\n\n\ndd6.icr.io\nIBM Cloud Paks content\n\n\nhttps://github.com/IBM/cloud-pak-cli/releases\nCloud Pak CLI\n\n\nhttps://mirror.openshift.com/pub/openshift-v4/x86_64/clients/\nOpenShift oc, oc-mirror and podman CLI\n\n\nhttps://github.com/IBM/cloud-pak/tree/master/repo/case\nIBM Cloud Pak CASE files\n\n\nmyibm.ibm.com\nIBM Registry API key access\n\n\nnvcr.io\nRequired to provide containerized GPUaccelerated software for AI, machine learning, deep learning, and highperformance computing (HPC) workloads.\n\n\nhttps://www.ibm.com/software/passportadvantage/pao_customer.html\nDownload software and order software, View Proof of Entitlement certificates.\n\n\n\n\n\n\nTable 1: API Load balancer To be determined\n\n\n\n\n\n\n\n\n\n\n\n\nPort\nBack-end machines (pool members)\nInternal\nExternal\nDescription\n\n\n\n\n6443\nBootstrap and control plane. You remove the bootstrap machine from the load balancer after the bootstrap machine initializes the cluster control plane. You must configure the /readyz endpoint for the API server health check probe.\nX\nX\nKubernetes API server\n\n\n22623\nBootstrap and control plane. You remove the bootstrap machine from the load balancer after the bootstrap machine initializes the cluster control plane.\nX\n\nMachine config server\n\n\n\n\n\n\n\n\n\nTable 2: Application Ingress Load balancer To be determined\n\n\n\n\n\n\n\n\n\n\n\n\nPort\nBack-end machines (pool members)\nInternal\nExternal\nDescription\n\n\n\n\n443\nThe machines that run the Ingress router pods, compute, or worker, by default.\nX\nX\nHTTPS traffic\n\n\n80\nThe machines that run the Ingress router pods, compute, or worker, by default.\nX\nX\nHTTP traffic",
    "crumbs": [
      "Deployment",
      "OpenShift"
    ]
  },
  {
    "objectID": "src/Deployment/1-OpenShift.html#pre-requirements",
    "href": "src/Deployment/1-OpenShift.html#pre-requirements",
    "title": "Step One",
    "section": "",
    "text": "Internet access for OpenShift Container Platform\nIn OpenShift Container Platform 4.14, you require access to the Internet to install your cluster.\nYou must have Internet access to:\n\nAccess the Red Hat OpenShift Cluster Manager page to download the installation program and perform subscription management. If the cluster has Internet access and you do not disable Telemetry, that service automatically entitles your cluster.\nAccess Quay.io to obtain the packages that are required to install your cluster.\nObtain the packages that are required to perform cluster updates.\nURLs listed below should be accessible from all the cluster nodes at port 443 and 80.\n\n\n\n\n\n\n\n\nURL\nFunction\n\n\n\n\nregistry.redhat.io\nProvides core container images\n\n\nquay.io\nProvides core container images\n\n\n*.quay.io\nProvides core container images\n\n\nsso.redhat.com\nThe https://console.redhat.com/openshift site uses authentication from sso.redhat.com\n\n\nopenshift.org\nProvides Red Hat Enterprise Linux CoreOS (RHCOS) images\n\n\nmirror.openshift.com\nRequired to access mirrored installation content and images. This site is also a source of release image signatures, although the Cluster Version Operator needs only a single functioning source.\n\n\nstorage.googleapis.com/openshift-release\nA source of release image signatures, although the Cluster Version Operator needs only a single functioning source.\n\n\napi.openshift.com\nRequired both for your cluster token and to check if updates are available for the cluster.\n\n\ncp.icr.io\nIBM Cloud Paks content\n\n\nicr.io\nIBM Cloud Paks content\n\n\ndocker.io\nIBM Cloud Paks content\n\n\ndd0.icr.io\nIBM Cloud Paks content\n\n\ndd2.icr.io\nIBM Cloud Paks content\n\n\ndd4.icr.io\nIBM Cloud Paks content\n\n\ndd6.icr.io\nIBM Cloud Paks content\n\n\nhttps://github.com/IBM/cloud-pak-cli/releases\nCloud Pak CLI\n\n\nhttps://mirror.openshift.com/pub/openshift-v4/x86_64/clients/\nOpenShift oc, oc-mirror and podman CLI\n\n\nhttps://github.com/IBM/cloud-pak/tree/master/repo/case\nIBM Cloud Pak CASE files\n\n\nmyibm.ibm.com\nIBM Registry API key access\n\n\nnvcr.io\nRequired to provide containerized GPUaccelerated software for AI, machine learning, deep learning, and highperformance computing (HPC) workloads.\n\n\nhttps://www.ibm.com/software/passportadvantage/pao_customer.html\nDownload software and order software, View Proof of Entitlement certificates.\n\n\n\n\n\n\nTable 1: API Load balancer To be determined\n\n\n\n\n\n\n\n\n\n\n\n\nPort\nBack-end machines (pool members)\nInternal\nExternal\nDescription\n\n\n\n\n6443\nBootstrap and control plane. You remove the bootstrap machine from the load balancer after the bootstrap machine initializes the cluster control plane. You must configure the /readyz endpoint for the API server health check probe.\nX\nX\nKubernetes API server\n\n\n22623\nBootstrap and control plane. You remove the bootstrap machine from the load balancer after the bootstrap machine initializes the cluster control plane.\nX\n\nMachine config server\n\n\n\n\n\n\n\n\n\nTable 2: Application Ingress Load balancer To be determined\n\n\n\n\n\n\n\n\n\n\n\n\nPort\nBack-end machines (pool members)\nInternal\nExternal\nDescription\n\n\n\n\n443\nThe machines that run the Ingress router pods, compute, or worker, by default.\nX\nX\nHTTPS traffic\n\n\n80\nThe machines that run the Ingress router pods, compute, or worker, by default.\nX\nX\nHTTP traffic",
    "crumbs": [
      "Deployment",
      "OpenShift"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html",
    "href": "src/Deployment/2-CP4D.html",
    "title": "Cloud Pak for Data",
    "section": "",
    "text": "The CLI can be downloaded from the following site.\nhttps://github.com/IBM/cpd-cli/releases\n\n\n\nCreate a new file name cpd_vars.sh and add the following:\n#===============================================================================\n# Cloud Pak for Data installation variables\n#===============================================================================\n\n# ------------------------------------------------------------------------------\n# Client workstation \n# ------------------------------------------------------------------------------\n# Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export CPD_CLI_MANAGE_WORKSPACE=&lt;enter a fully qualified directory&gt;\n# export OLM_UTILS_LAUNCH_ARGS=&lt;enter launch arguments&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cluster\n# ------------------------------------------------------------------------------\n\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=&lt;enter your deployment type&gt;\nexport IMAGE_ARCH=&lt;enter your cluster architecture&gt;\n# export OCP_USERNAME=&lt;enter your username&gt;\n# export OCP_PASSWORD=&lt;enter your password&gt;\n# export OCP_TOKEN=&lt;enter your token&gt;\nexport SERVER_ARGUMENTS=\"--server=${OCP_URL}\"\n# export LOGIN_ARGUMENTS=\"--username=${OCP_USERNAME} --password=${OCP_PASSWORD}\"\n# export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\"\nexport CPDM_OC_LOGIN=\"cpd-cli manage login-to-ocp ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\nexport OC_LOGIN=\"oc login ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\n\n\n# ------------------------------------------------------------------------------\n# Proxy server\n# ------------------------------------------------------------------------------\n\n# export PROXY_HOST=&lt;enter your proxy server hostname&gt;\n# export PROXY_PORT=&lt;enter your proxy server port number&gt;\n# export PROXY_USER=&lt;enter your proxy server username&gt;\n# export PROXY_PASSWORD=&lt;enter your proxy server password&gt;\n\n\n# ------------------------------------------------------------------------------\n# Projects\n# ------------------------------------------------------------------------------\n\nexport PROJECT_CERT_MANAGER=&lt;enter your certificate manager project&gt;\nexport PROJECT_LICENSE_SERVICE=&lt;enter your License Service project&gt;\nexport PROJECT_SCHEDULING_SERVICE=&lt;enter your scheduling service project&gt;\n# export PROJECT_IBM_EVENTS=&lt;enter your IBM Events Operator project&gt;\n# export PROJECT_PRIVILEGED_MONITORING_SERVICE=&lt;enter your privileged monitoring service project&gt;\nexport PROJECT_CPD_INST_OPERATORS=&lt;enter your Cloud Pak for Data operator project&gt;\nexport PROJECT_CPD_INST_OPERANDS=&lt;enter your Cloud Pak for Data operand project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED=&lt;enter your tethered project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED_LIST=&lt;a comma-separated list of tethered projects&gt;\n\n\n\n# ------------------------------------------------------------------------------\n# Storage\n# ------------------------------------------------------------------------------\n\nexport STG_CLASS_BLOCK=&lt;RWO-storage-class-name&gt;\nexport STG_CLASS_FILE=&lt;RWX-storage-class-name&gt;\n\n# ------------------------------------------------------------------------------\n# IBM Entitled Registry\n# ------------------------------------------------------------------------------\n\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\n# ------------------------------------------------------------------------------\n# Private container registry\n# ------------------------------------------------------------------------------\n# Set the following variables if you mirror images to a private container registry.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cloud Pak for Data version\n# ------------------------------------------------------------------------------\n\nexport VERSION=5.1.3\n\n\n# ------------------------------------------------------------------------------\n# Components\n# ------------------------------------------------------------------------------\n\nexport COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_platform\n# export COMPONENTS_TO_SKIP=&lt;component-ID-1&gt;,&lt;component-ID-2&gt;\nThe following variables will need to be updated to include the OCP URL, OpenShift type (usually self-managed unless its on a specific cloud provider), and Image archetype such as “amd64”\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=self-managed\nexport IMAGE_ARCH=amd64\nSet the Project names for the different services, here are a few suggestions:\nexport PROJECT_CERT_MANAGER=ibm-cert-manager\nexport PROJECT_LICENSE_SERVICE=cpd-license-server\nexport PROJECT_SCHEDULING_SERVICE=cpd-scheduling\nexport PROJECT_CPD_INST_OPERATORS=cpd-operators\nexport PROJECT_CPD_INST_OPERANDS=cpd-instance\nSet your block storage class and file storage class, such as ocs-storagecluster-ceph-rbd and ocs-storagecluster-cephfs .\nexport STG_CLASS_BLOCK=ocs-storagecluster-ceph-rbd\nexport STG_CLASS_FILE=ocs-storagecluster-cephfs\nSet your private registry details:\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\nAdd your IBM Entitlement key to the following variable:\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\nRun the following command any time you update the environment variables file:\nsource cpd_vars.sh\n\n\n\n\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --password=${OCP_PASSWORD}\nor\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --token=${OCP_TOKEN}\n\n\n\ncpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key=${IBM_ENTITLEMENT_KEY}",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html#cloud-pak-for-data-install-preparation",
    "href": "src/Deployment/2-CP4D.html#cloud-pak-for-data-install-preparation",
    "title": "Cloud Pak for Data",
    "section": "",
    "text": "The CLI can be downloaded from the following site.\nhttps://github.com/IBM/cpd-cli/releases\n\n\n\nCreate a new file name cpd_vars.sh and add the following:\n#===============================================================================\n# Cloud Pak for Data installation variables\n#===============================================================================\n\n# ------------------------------------------------------------------------------\n# Client workstation \n# ------------------------------------------------------------------------------\n# Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export CPD_CLI_MANAGE_WORKSPACE=&lt;enter a fully qualified directory&gt;\n# export OLM_UTILS_LAUNCH_ARGS=&lt;enter launch arguments&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cluster\n# ------------------------------------------------------------------------------\n\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=&lt;enter your deployment type&gt;\nexport IMAGE_ARCH=&lt;enter your cluster architecture&gt;\n# export OCP_USERNAME=&lt;enter your username&gt;\n# export OCP_PASSWORD=&lt;enter your password&gt;\n# export OCP_TOKEN=&lt;enter your token&gt;\nexport SERVER_ARGUMENTS=\"--server=${OCP_URL}\"\n# export LOGIN_ARGUMENTS=\"--username=${OCP_USERNAME} --password=${OCP_PASSWORD}\"\n# export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\"\nexport CPDM_OC_LOGIN=\"cpd-cli manage login-to-ocp ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\nexport OC_LOGIN=\"oc login ${SERVER_ARGUMENTS} ${LOGIN_ARGUMENTS}\"\n\n\n# ------------------------------------------------------------------------------\n# Proxy server\n# ------------------------------------------------------------------------------\n\n# export PROXY_HOST=&lt;enter your proxy server hostname&gt;\n# export PROXY_PORT=&lt;enter your proxy server port number&gt;\n# export PROXY_USER=&lt;enter your proxy server username&gt;\n# export PROXY_PASSWORD=&lt;enter your proxy server password&gt;\n\n\n# ------------------------------------------------------------------------------\n# Projects\n# ------------------------------------------------------------------------------\n\nexport PROJECT_CERT_MANAGER=&lt;enter your certificate manager project&gt;\nexport PROJECT_LICENSE_SERVICE=&lt;enter your License Service project&gt;\nexport PROJECT_SCHEDULING_SERVICE=&lt;enter your scheduling service project&gt;\n# export PROJECT_IBM_EVENTS=&lt;enter your IBM Events Operator project&gt;\n# export PROJECT_PRIVILEGED_MONITORING_SERVICE=&lt;enter your privileged monitoring service project&gt;\nexport PROJECT_CPD_INST_OPERATORS=&lt;enter your Cloud Pak for Data operator project&gt;\nexport PROJECT_CPD_INST_OPERANDS=&lt;enter your Cloud Pak for Data operand project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED=&lt;enter your tethered project&gt;\n# export PROJECT_CPD_INSTANCE_TETHERED_LIST=&lt;a comma-separated list of tethered projects&gt;\n\n\n\n# ------------------------------------------------------------------------------\n# Storage\n# ------------------------------------------------------------------------------\n\nexport STG_CLASS_BLOCK=&lt;RWO-storage-class-name&gt;\nexport STG_CLASS_FILE=&lt;RWX-storage-class-name&gt;\n\n# ------------------------------------------------------------------------------\n# IBM Entitled Registry\n# ------------------------------------------------------------------------------\n\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\n# ------------------------------------------------------------------------------\n# Private container registry\n# ------------------------------------------------------------------------------\n# Set the following variables if you mirror images to a private container registry.\n#\n# To export these variables, you must uncomment each command in this section.\n\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\n\n\n# ------------------------------------------------------------------------------\n# Cloud Pak for Data version\n# ------------------------------------------------------------------------------\n\nexport VERSION=5.1.3\n\n\n# ------------------------------------------------------------------------------\n# Components\n# ------------------------------------------------------------------------------\n\nexport COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_platform\n# export COMPONENTS_TO_SKIP=&lt;component-ID-1&gt;,&lt;component-ID-2&gt;\nThe following variables will need to be updated to include the OCP URL, OpenShift type (usually self-managed unless its on a specific cloud provider), and Image archetype such as “amd64”\nexport OCP_URL=&lt;enter your Red Hat OpenShift Container Platform URL&gt;\nexport OPENSHIFT_TYPE=self-managed\nexport IMAGE_ARCH=amd64\nSet the Project names for the different services, here are a few suggestions:\nexport PROJECT_CERT_MANAGER=ibm-cert-manager\nexport PROJECT_LICENSE_SERVICE=cpd-license-server\nexport PROJECT_SCHEDULING_SERVICE=cpd-scheduling\nexport PROJECT_CPD_INST_OPERATORS=cpd-operators\nexport PROJECT_CPD_INST_OPERANDS=cpd-instance\nSet your block storage class and file storage class, such as ocs-storagecluster-ceph-rbd and ocs-storagecluster-cephfs .\nexport STG_CLASS_BLOCK=ocs-storagecluster-ceph-rbd\nexport STG_CLASS_FILE=ocs-storagecluster-cephfs\nSet your private registry details:\n# export PRIVATE_REGISTRY_LOCATION=&lt;enter the location of your private container registry&gt;\n# export PRIVATE_REGISTRY_PUSH_USER=&lt;enter the username of a user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PUSH_PASSWORD=&lt;enter the password of the user that can push to the registry&gt;\n# export PRIVATE_REGISTRY_PULL_USER=&lt;enter the username of a user that can pull from the registry&gt;\n# export PRIVATE_REGISTRY_PULL_PASSWORD=&lt;enter the password of the user that can pull from the registry&gt;\nAdd your IBM Entitlement key to the following variable:\nexport IBM_ENTITLEMENT_KEY=&lt;enter your IBM entitlement API key&gt;\n\n\nRun the following command any time you update the environment variables file:\nsource cpd_vars.sh\n\n\n\n\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --password=${OCP_PASSWORD}\nor\ncpd-cli manage login-to-ocp --username=${OCP_USERNAME} --token=${OCP_TOKEN}\n\n\n\ncpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key=${IBM_ENTITLEMENT_KEY}",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html#install-cp4d-pre-requisites",
    "href": "src/Deployment/2-CP4D.html#install-cp4d-pre-requisites",
    "title": "Cloud Pak for Data",
    "section": "Install CP4D Pre-Requisites",
    "text": "Install CP4D Pre-Requisites\n\nCertificate Manager & License Server\nThe following command will install the certificate manager and the License Server:\ncpd-cli manage apply-cluster-components \\\n--release=${VERSION} \\\n--license_acceptance=true \\\n--licensing_ns=${PROJECT_LICENSE_SERVICE}\n\n\nScheduling Service\nThe following command will install the Scheduling service:\ncpd-cli manage apply-scheduler \\\n--release=${VERSION} \\\n--license_acceptance=true \\\n--scheduler_ns=${PROJECT_SCHEDULING_SERVICE}\n\n\nCreate required namespaces\nThe following commands will create all the necessary namespaces:\noc new-project ${PROJECT_CPD_INST_OPERATORS}\noc new-project ${PROJECT_CPD_INST_OPERANDS}\n\n\nApply Permissions\nThe following commands will apply the required permissions for the CP4D Topology:\ncpd-cli manage authorize-instance-topology \\\n--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/Deployment/2-CP4D.html#install-cloud-pak-for-data",
    "href": "src/Deployment/2-CP4D.html#install-cloud-pak-for-data",
    "title": "Cloud Pak for Data",
    "section": "Install Cloud Pak for Data",
    "text": "Install Cloud Pak for Data\n\nCloud Pak Foundation Services\nThe following command will install the Cloud Pak foundation services:\ncpd-cli manage setup-instance-topology \\\n--release=${VERSION} \\\n--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \\\n--license_acceptance=true \\\n--block_storage_class=${STG_CLASS_BLOCK}\n\n\nInstall operators\nThe following command will install the Cloud Pak for Data operators which are required to install services:\ncpd-cli manage apply-olm \\\n--release=${VERSION} \\\n--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \\\n--components=${COMPONENTS}\n\n\nInstall operands\nThe following command will install the Cloud Pak for Data base software.\ncpd-cli manage apply-cr \\\n--release=${VERSION} \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \\\n--components=cpd_platform \\\n--block_storage_class=${STG_CLASS_BLOCK} \\\n--file_storage_class=${STG_CLASS_FILE} \\\n--license_acceptance=true\n\n\nConfirm Operands are Completed:\ncpd-cli manage get-cr-status \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}\n\n\nRetrieve Login Details\nRun the following commands to get the Admin username and password for the Cloud Pak for Data install:\ncpd-cli manage get-cpd-instance-details \\\n--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \\\n--get_admin_initial_credentials=true",
    "crumbs": [
      "Deployment",
      "Cloud Pak for Data Installation"
    ]
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IBM Knowledge Catalog on GCP",
    "section": "",
    "text": "SL",
    "crumbs": [
      "IBM Knowledge Catalog on GCP"
    ]
  },
  {
    "objectID": "index.html#description-ibm-knowledge-catalog-on-gcp",
    "href": "index.html#description-ibm-knowledge-catalog-on-gcp",
    "title": "IBM Knowledge Catalog on GCP",
    "section": "Description: “IBM Knowledge Catalog on GCP”",
    "text": "Description: “IBM Knowledge Catalog on GCP”",
    "crumbs": [
      "IBM Knowledge Catalog on GCP"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "IBM Knowledge Catalog on GCP",
    "section": "The Why",
    "text": "The Why\n(High-level description of the business challenges and client pain points)\n\nProblem Details\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.\n\n\nAdditional Context\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.",
    "crumbs": [
      "IBM Knowledge Catalog on GCP"
    ]
  },
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Best Practices\n(Capture the main takeaways and results of the project)"
  },
  {
    "objectID": "src/Deployment/3-IKC.html",
    "href": "src/Deployment/3-IKC.html",
    "title": "IBM Knowledge Catalog",
    "section": "",
    "text": "Go to OperatorHub\nSearch for “Node Feature Discovery”\nInstall\n\n\n\n\n\nGo to Installed Operators\nSelect “Node Feature Discovery”\nSelect the box “Provided APIs”\nSelect “Create Instance”\nReview the values\nSelect “Create”\n\n\n\n\n\nGo to OperatorHub\nSearch for “Nvidia GPU Operator”\nInstall\n\n\n\n\n\nGo to “Installed Operators”\nClick on “Nvidia GPU Operator”\nSelect “ClusterPolicy” tab\nClick “Create ClusterPolicy”\nClick “Create”\n\n\n\n\nCreate redhat-ods-operator project:\noc new-project redhat-ods-operator\nCreate the rhods-operator operator group in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nEOF\nCreate the rhods-operator operator subscription in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nspec:\n  name: rhods-operator\n  channel: stable-2.13\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n  config:\n     env:\n        - name: \"DISABLE_DSC_CONFIG\"\nEOF\nCreate a DSC Initialization (DSCInitialization) object named default-dsci in the redhat-ods-monitoring project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: dscinitialization.opendatahub.io/v1\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  applicationsNamespace: redhat-ods-applications\n  monitoring:\n    managementState: Managed\n    namespace: redhat-ods-monitoring\n  serviceMesh:\n    managementState: Removed\n  trustedCABundle:\n    managementState: Managed\n    customCABundle: \"\"\nEOF\nCheck the status of the rhods-operator-* pod in the redhat-ods-operator project:\noc get pods -n redhat-ods-operator\nConfirm that the pod is Running. The command returns a response with the following format:\nNAME                              READY   STATUS    RESTARTS   AGE\nrhods-operator-56c85d44c9-vtk74   1/1     Running   0          3h57m\nCheck the phase of the DSC Initialization (DSCInitialization) object:\noc get dscinitialization\nConfirm that the object is Ready. The command returns a response with the following format:\nNAME           AGE     PHASE\ndefault-dsci   4d18h   Ready\nCreate a Data Science Cluster (DataScienceCluster) object named default-dsc:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: datasciencecluster.opendatahub.io/v1\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    codeflare:\n      managementState: Removed\n    dashboard:\n      managementState: Removed\n    datasciencepipelines:\n      managementState: Removed\n    kserve:\n      managementState: Managed\n      defaultDeploymentMode: RawDeployment\n      serving:\n        managementState: Removed\n        name: knative-serving\n    kueue:\n      managementState: Removed\n    modelmeshserving:\n      managementState: Removed\n    ray:\n      managementState: Removed\n    trainingoperator:\n      managementState: Managed\n    trustyai:\n      managementState: Removed\n    workbenches:\n      managementState: Removed\nEOF\nWait for the Data Science Cluster object to be Ready. To check the status of the object, run:\noc get datasciencecluster default-dsc -o jsonpath='\"{.status.phase}\" {\"\\n\"}'\nConfirm that the status of the following pods in the redhat-ods-applications project are Running: kserve-controller-manager-* pod kubeflow-training-operator-* pod odh-model-controller-* pod\noc get pods -n redhat-ods-applications\nThe command returns a response with the following format:\nNAME                                         READY   STATUS      RESTARTS   AGE\nkserve-controller-manager-57796d5b44-sh9n5   1/1     Running     0          4m57s\nkubeflow-training-operator-7b99d5584c-rh5hb  1/1     Running     0          4m57s\nEdit the inferenceservice-config configuration map in the redhat-ods-applications project: Log in to the Red Hat OpenShift Container Platform web console as a cluster administrator. From the navigation menu, select Workloads &gt; Configmaps. From the Project list, select redhat-ods-applications. Click the inferenceservice-config resource. Then, open the YAML tab. In the metadata.annotations section of the file, add opendatahub.io/managed: ‘false’:\nmetadata:\n  annotations:\n    internal.config.kubernetes.io/previousKinds: ConfigMap\n    internal.config.kubernetes.io/previousNames: inferenceservice-config\n    internal.config.kubernetes.io/previousNamespaces: opendatahub\n    opendatahub.io/managed: 'false'\nFind the following entry in the file:\n\"domainTemplate\": \"{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}\",\nUpdate the value of the domainTemplate field to “example.com”:\n\"domainTemplate\": \"example.com\",\nclick save.",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html#pre-requirements",
    "href": "src/Deployment/3-IKC.html#pre-requirements",
    "title": "IBM Knowledge Catalog",
    "section": "",
    "text": "Go to OperatorHub\nSearch for “Node Feature Discovery”\nInstall\n\n\n\n\n\nGo to Installed Operators\nSelect “Node Feature Discovery”\nSelect the box “Provided APIs”\nSelect “Create Instance”\nReview the values\nSelect “Create”\n\n\n\n\n\nGo to OperatorHub\nSearch for “Nvidia GPU Operator”\nInstall\n\n\n\n\n\nGo to “Installed Operators”\nClick on “Nvidia GPU Operator”\nSelect “ClusterPolicy” tab\nClick “Create ClusterPolicy”\nClick “Create”\n\n\n\n\nCreate redhat-ods-operator project:\noc new-project redhat-ods-operator\nCreate the rhods-operator operator group in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nEOF\nCreate the rhods-operator operator subscription in the redhat-ods-operator project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nspec:\n  name: rhods-operator\n  channel: stable-2.13\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n  config:\n     env:\n        - name: \"DISABLE_DSC_CONFIG\"\nEOF\nCreate a DSC Initialization (DSCInitialization) object named default-dsci in the redhat-ods-monitoring project:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: dscinitialization.opendatahub.io/v1\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  applicationsNamespace: redhat-ods-applications\n  monitoring:\n    managementState: Managed\n    namespace: redhat-ods-monitoring\n  serviceMesh:\n    managementState: Removed\n  trustedCABundle:\n    managementState: Managed\n    customCABundle: \"\"\nEOF\nCheck the status of the rhods-operator-* pod in the redhat-ods-operator project:\noc get pods -n redhat-ods-operator\nConfirm that the pod is Running. The command returns a response with the following format:\nNAME                              READY   STATUS    RESTARTS   AGE\nrhods-operator-56c85d44c9-vtk74   1/1     Running   0          3h57m\nCheck the phase of the DSC Initialization (DSCInitialization) object:\noc get dscinitialization\nConfirm that the object is Ready. The command returns a response with the following format:\nNAME           AGE     PHASE\ndefault-dsci   4d18h   Ready\nCreate a Data Science Cluster (DataScienceCluster) object named default-dsc:\ncat &lt;&lt;EOF |oc apply -f -\napiVersion: datasciencecluster.opendatahub.io/v1\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    codeflare:\n      managementState: Removed\n    dashboard:\n      managementState: Removed\n    datasciencepipelines:\n      managementState: Removed\n    kserve:\n      managementState: Managed\n      defaultDeploymentMode: RawDeployment\n      serving:\n        managementState: Removed\n        name: knative-serving\n    kueue:\n      managementState: Removed\n    modelmeshserving:\n      managementState: Removed\n    ray:\n      managementState: Removed\n    trainingoperator:\n      managementState: Managed\n    trustyai:\n      managementState: Removed\n    workbenches:\n      managementState: Removed\nEOF\nWait for the Data Science Cluster object to be Ready. To check the status of the object, run:\noc get datasciencecluster default-dsc -o jsonpath='\"{.status.phase}\" {\"\\n\"}'\nConfirm that the status of the following pods in the redhat-ods-applications project are Running: kserve-controller-manager-* pod kubeflow-training-operator-* pod odh-model-controller-* pod\noc get pods -n redhat-ods-applications\nThe command returns a response with the following format:\nNAME                                         READY   STATUS      RESTARTS   AGE\nkserve-controller-manager-57796d5b44-sh9n5   1/1     Running     0          4m57s\nkubeflow-training-operator-7b99d5584c-rh5hb  1/1     Running     0          4m57s\nEdit the inferenceservice-config configuration map in the redhat-ods-applications project: Log in to the Red Hat OpenShift Container Platform web console as a cluster administrator. From the navigation menu, select Workloads &gt; Configmaps. From the Project list, select redhat-ods-applications. Click the inferenceservice-config resource. Then, open the YAML tab. In the metadata.annotations section of the file, add opendatahub.io/managed: ‘false’:\nmetadata:\n  annotations:\n    internal.config.kubernetes.io/previousKinds: ConfigMap\n    internal.config.kubernetes.io/previousNames: inferenceservice-config\n    internal.config.kubernetes.io/previousNamespaces: opendatahub\n    opendatahub.io/managed: 'false'\nFind the following entry in the file:\n\"domainTemplate\": \"{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}\",\nUpdate the value of the domainTemplate field to “example.com”:\n\"domainTemplate\": \"example.com\",\nclick save.",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html#installation",
    "href": "src/Deployment/3-IKC.html#installation",
    "title": "IBM Knowledge Catalog",
    "section": "Installation",
    "text": "Installation",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  },
  {
    "objectID": "src/Deployment/3-IKC.html#configuration",
    "href": "src/Deployment/3-IKC.html#configuration",
    "title": "IBM Knowledge Catalog",
    "section": "Configuration",
    "text": "Configuration",
    "crumbs": [
      "Deployment",
      "IBM Knowledge Catalog Installation"
    ]
  }
]